{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width  petal_length  petal_width species\n",
      "0           5.1          3.5           1.4          0.2  setosa\n",
      "1           4.9          3.0           1.4          0.2  setosa\n",
      "2           4.7          3.2           1.3          0.2  setosa\n",
      "3           4.6          3.1           1.5          0.2  setosa\n",
      "4           5.0          3.6           1.4          0.2  setosa\n",
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "import seaborn as sns # seaborn을 불러오고 SNS로 축약\n",
    "import numpy as np\n",
    "\n",
    "iris = sns.load_dataset('iris')  # iris라는 변수명으로 Iris data를 download\n",
    "print(iris.head())\n",
    "X = iris.drop('species', axis=1) # 'species'열을 drop하고 input X를 정의\n",
    "y = iris['species']\n",
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# y data를 범주형으로 변환\n",
    "from sklearn.preprocessing import LabelEncoder    # LabelEncoder() method를 불러옴\n",
    "\n",
    "classle = LabelEncoder()\n",
    "y = classle.fit_transform(iris['species'].values) # species 열의 문자열을 categorical 값으로 전환\n",
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 data를 training set과 test set으로 split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
    "                                                    random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표준화\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "Logit = LogisticRegression(C=1e2, random_state=1,\n",
    "                          multi_class='ovr', max_iter=200)  # C = 1/λ. 디폴트: L2, One-versus-Rest\n",
    "                        # C값이 클수록 λ이 작아지기 때문에 규제화의 강도가 줄어든다.\n",
    "                        # ovr: one-vs-rest로 가장 큰 확률을 가진 범주를 해당 관측치에 할당 (multi_class='multinomial'도 가능)\n",
    "l_1=Logit.fit(X_train_std, y_train)\n",
    "y_train_pred = Logit.predict(X_train_std)\n",
    "y_test_pred = Logit.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9777777777777777\n",
      "0.9428571428571428\n"
     ]
    }
   ],
   "source": [
    "# Accuracy score (정밀도=전체 표본에서 정확하게 예측을 맞춘 비율)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(y_test,y_test_pred))    \n",
    "print(accuracy_score(y_train,y_train_pred))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15  0  0]\n",
      " [ 0 15  0]\n",
      " [ 0  1 14]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test, y_test_pred))  # Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 0 2 1]\n",
      "[[5.92893708e-10 5.87048155e-04 9.99412951e-01]\n",
      " [9.98963464e-01 1.03653599e-03 1.98612888e-20]\n",
      " [9.99802215e-01 1.97784985e-04 3.43495956e-21]\n",
      " [3.09188776e-05 4.37402877e-01 5.62566204e-01]\n",
      " [2.14214878e-05 9.99312629e-01 6.65949451e-04]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zoeyki/opt/anaconda3/envs/py_tf/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "Logit = LogisticRegression(C=1e2, random_state=1, max_iter=200, multi_class = 'ovr')  # C = 1/λ. 디폴트: L2, Auto\n",
    "# One-versus-rest로 진행하는 경우 multi_class='ovr'이라고 명시해줘야 함.\n",
    "Logit.fit(X_train, y_train)\n",
    "y_train_pred = Logit.predict(X_train)\n",
    "y_test_pred = Logit.predict(X_test)\n",
    "y_test_pred_proba=Logit.predict_proba(X_test)  # predict_proba: predict probability\n",
    "print(y_test_pred[:5])\n",
    "print(y_test_pred_proba[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9809523809523809\n",
      "[[15  0  0]\n",
      " [ 0 15  0]\n",
      " [ 0  0 15]]\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test,y_test_pred))   \n",
    "print(accuracy_score(y_train,y_train_pred)) \n",
    "print(confusion_matrix(y_test, y_test_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습된 로지스틱 회귀모형을 저장하고 다시 불러내어 데이터에 직접 적용하기로 한다. <br>\n",
    "save.pkl 파일에 저장한 뒤, joblib.load() 함수를 이용해 이를 객체화하고 predict하는 함수 등을 적용해볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in /Users/zoeyki/opt/anaconda3/lib/python3.8/site-packages (1.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[15  0  0]\n",
      " [ 0 15  0]\n",
      " [ 0  0 15]]\n",
      "[2 0 0 2 1 1 2 1 2 0 0 2 0 1 0 1 2 1 1 2 2 0 1 2 1 1 1 2 0 2 0 0 1 1 2 2 0\n",
      " 0 0 1 2 2 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(Logit,'save.pkl')\n",
    "logit_from_joblib=joblib.load('save.pkl')\n",
    "logit_pred=logit_from_joblib.predict(X_test)\n",
    "print(accuracy_score(y_test,logit_pred))\n",
    "print(confusion_matrix(y_test,logit_pred))\n",
    "print(logit_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
